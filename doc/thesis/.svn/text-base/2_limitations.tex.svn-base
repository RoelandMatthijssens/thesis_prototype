%!TEX root = thesis.tex

\chapter{Limitations of Current Linking Techniques} \label{cha:Limitations of Linking Techniques}
	\paragraph{}
	In this chapter we take a closer look at the shortcomings of the standard HTML linking techniques. We point out why these shortcomings have a serious negative effect on the potential of the World Wide Web. We also elaborate on the reasons why these techniques became the standard way of linking, as well as why these techniques have not been updated over the last couple of decades.

%HISTORY {{{
\section{The History of The Web} \label{sec:The History Of The Web}
	\paragraph{}
	In 1980 Tim Berners-Lee was hired as an independent contractor for CERN where he was working on ENQUIRE\cite{caygill1999meno}, a personal database of people and software models. ENQUIRE was a simple hypertext program that had the same basic ideas of the World Wide Web and is therefore often referred to as its predecessor. However, many years before Tim Berners-Lee's work on what eventually became the World Wide Web, an American science-fiction writer named Murray Leinster wrote \dquote{A Logic Named Joe}, a short story published in 1946. The story describes the concept of a home-based global information system that had many of the same characteristics as the current Web.
	\paragraph{}
	In 1984 Tim Berners-Lee noticed a problem with CERN's information presentation. Physicists from all over the world needed to share their research with each other, but the lack of common machines and presentation software made this a cumbersome task. Therefore in 1989 he wrote a proposal for a large interlinked hypertext database~\cite{berners1989information}, but at the time this generated little interest. His boss Mike Sendalli however, believed in his ideas and encouraged him to start developing his system. 
	\paragraph{}
	By 1990 Berners-Lee had finished all of the building blocks of his World Wide Web. He had already created the HyperText Transfer Protocol~\cite{berners1992http} (HTTP) as well as the HyperText Markup Language~\cite{berners1992world1} (HTML). By that time, he had also created a first web browser, at that time named \squote{WorldWideWeb}~\cite{berners1992world2} and later renamed to \squote{Nexus}, the first HTTP server software and the first web server. As a proof of concept he also created a description of his project in the form of a website on his World Wide Web. The first version of HTML was a hybrid of SGML~\cite{van1994practical}, an already well accepted mark-up language, combined with the ability to link documents.
	\paragraph{}
	August 6, 1991 marked the d√©but of the Web as a publicly available service when Berners-Lee posted a short summary of his World Wide Web project. Tim Berners-Lee came from a research background, so he naturally opened up the debate for the features that HTML should provide to the public. A month later he started the WWW talk, a discussion group where Web enthusiasts could exchange ideas and insights. Soon after, HTML+ was composed, a richer version of the original HTML, that was more appropriate for mass consumption. 
	\paragraph{}
	In 1993, when HTML began to gain some momentum in the academic world, a couple of different web browsers were created. Lynx\footnote{\url{http://lynx.browser.org}}, a text-based browser for the DOS operating system, was a real asset to the development and open discussion of the HTML standard. Meanwhile the developers from Hewlett-Packard Labs were creating the Arena\footnote{\url{http://www.w3.org/Arena/}} browser. The Arena browser intended to demonstrate a set of newly invented features but many of these features were not standard HTML. In April 1993, NSCA released their Mosaic\footnote{\url{http://www.ncsa.illinois.edu/Projects/mosaic.html}} browser, further extending Tim Berners-Lee's specifications. The Mosaic browser got a lot of negative feedback from academics and software engineers who would argue that many of the added features were very ad hoc and not properly designed.
	\paragraph{}
	The increasing amount of new features that were popping up in commercially designed browsers led the discussions away from the academic world. The commercial interests were to a greater extend directed at adding new features and adding functionality, and were less well designed. Many features could be described as hacks in the system to allow for additional functionality.
	\paragraph{}
	Now that all these different browsers added their own bits of HTML the language lost its clear definition. In July 1994 all wide spread HTML tags were collected and a new broader HTML 2.0 standard was introduced. An important note is that almost all of the newly added tags originated from a commercial background and not from an academic one. Because of the relative popularity of these tags, removing them or changing their behaviour would have resulted in breaking a lot of websites. Instead of causing such a widespread user dissatisfaction the originally intended elegance got tainted by commercial influences.
	\paragraph{}
	Later that same year, the very powerful and immensely popular Netscape\footnote{\url{http://www.w3schools.com/browsers/browsers_netscape.asp}} browser was released. Being such a popular browser, it started designing its own HTML tags without first openly discussing the changes with the Web community. Netscape was rarely seen at the WWW conferences, but was, for a long time, the driving force behind the HTML standard. As a result, the HTML language started to degenerate again.
	\paragraph{}
	Another attempt at introducing a single standard was made in 1995 when a draft for HTML 3.0 was released. But as expected, the outcome was much the same. All important web browsers chose to implement a certain subset of the standard and extending the standard again to their own liking, claiming the browser was HTML 3.0 compliant. In truth the actual HTML 3.0 standard was never released and remained perpetually in \squote{draft}, but amidst all the confusion this mattered little. HTML standards were a mess.
	\paragraph{}
	Thereafter, things went from bad to worse. Upcoming Microsoft's Internet Explorer was again redefining its own HTML while Netscape introduced frames. In short the HTML work group that was set up to manage the standard was overwhelmed by the browser vendors. HTML 3.2, which was released in 1997, did not do much better than its predecessors and it was not until 1998 and version 4.0 that things were clearing up.
	\paragraph{}
	After contacting the representatives of the major browser companies to meet as a small group dedicated to standardising HTML things went better. Cascading Style Sheets (CSS) were introduced, Scripting support was added and HTML was back on a stable track. Finally, in 2008 a first public draft of HTML5 was released as a rolling standard.
%HISTORY }}}

\section{Limitations} \label{sec:Limitations}
	\paragraph{}
	The important lesson to take away from the brief history of the Web is the way the web standards have evolved over time and the lopsided influences that shaped its course. At first, academic needs drove the standard forwards, but once the Web became popular and got more momentum, the driving force behind the changes shifted to the browser vendors, and commercially oriented companies. The big influences that are currently steering the direction of the HTML standards have lost sight of the elegant concepts that were the initial inspiration of the Web. As a result, the current standard is littered with outdated and archaic techniques that are kept for backwards compatibility.
	\paragraph{}
	Most importantly, we are still linking web pages together with technology that originated from the short term practical solution to an urgent practical problem of almost 30 years ago. The mainly used technique to link two documents together on the Web is still the HTML anchor tag: \code{<a href="destination"></a>}.
	\subsection{Wasted Potential} \label{sub:Wasted Potential}
	\paragraph{}
	One big issue with these anchor tags is that they can only be placed directly embedded in the source of the HTML. This means that if a new web page is created, we would have to edit the source code of existing documents to link them together. But HTML was designed as a mark-up language, intended to describe how a document is structured but through historic events styles, scripts, images, links, etc.\ were all thrown together in the HTML language. Since HTML 3.2, an evolution has been going on to separate the different concerns, as was originally intended. Cascading Style Sheets and JavaScript were introduced to remedy some of these issues, but hyperlinks still need to be embedded in the HTML structure.
	\paragraph{}
	A second problem with the original anchor tag, introduced in HTML 2.0, is that it only supports unidirectional links with a single source and a single destination. The destination can be a different document, an anchor tag on the same web page~(\code{<a href="\#anchor">}), or a combination of the two~(\code{<a href="destination\#anchor">}). The destination of the link is specified by the \code{href} attribute of the anchor tag, while the source is the text between the opening and closing anchor tags.
	\paragraph{}
	Unidirectional links are another big limitation on web linking. The importance of academic papers is often rated based on the amount of citations on the article and it is important to know in which documents a paper is cited. One would assume that this idea would have percolated into the Web's standards but this is not the case. In fact finding out all backlinks to a specific website is incredibly difficult~\cite{bjorneborn2004toward}. Big Internet companies are still very interested in knowing what pages are linking to them and most commercial search engines provide tools to find these backlinks. For example, Google can be searched using \dquote{link:www.vub.ac.be} to get all the websites in Google's cache that link to the VUB's home page. Additionally many search engines use the amount of backlinks to a page as an indication of the popularity of the page, and will sort the search results according to this popularity\footnote{\url{http://www.whitelines.nl/html/google-page-rank.html}}. But these search engines would have to do extensive work in order to get a good estimation of the amount of backlinks which would not have been the case if HTML would support bidirectional links in the first place.
	\paragraph{}
	The fact that an HTML link can only point to a single destination is a severe limitation as well. In many cases, a single sentence has a semantic relation to a couple of different resources where more information can be found. One link would for example point to the relevant Wikipedia page, while the author of the page would also want to link to a couple of interesting articles or papers. In order to achieve this with the current standards additional text would have to be added to the page for the sole purpose of making it into a link. So the mark-up language would have to be abused to add links to multiple different documents.
	\paragraph{}
	The same could be said for the limitation of single source links. A blog author might be talking about a specific topic, and throughout the article he would refer an important paper related to the topic. In order to do this, he would have to create the same anchor tags in several places pointing to the same document. But this repetition of code is prone to mistakes, and it will be cumbersome to edit these links if the paper they point to would change location. If the HTML standard would allow authors to create multi-source links this would not be a problem.
	
\section{Distinction Between Author and Consumer} \label{sec:Distinction Between Author and Consumer}
	\paragraph{}
	Today's Web is becoming more and more consumer based and since the notion of Web 2.0 became popular consumers have been allowed to produce content as well. Blog websites are now commonplace on the Internet, and big Online companies such as Twitter and Facebook are making millions by allowing their users to create and share content. The rise of social network websites, web-logging websites, wikis and video sharing websites emphasises the evolution towards a more consumer based Web.
	\paragraph{}
	A very important aspect of the Web is the fact that content needs to be linked together. It is therefore only logical to assume that the current trend will eventually change the passive reader and consumer of links into an active creator of hyperlinks between web pages. But this is not yet the case, there is still a very clear distinction between the authors of a web pages, who are able to embed links in their content, and the readers or consumers of the web content who are unable to add links to the content.
	\paragraph{}
	There are plenty of research topics that were deemed to be a dead end, but after a couple of years new insights would reignite the research and show interesting new findings. Of course, papers describing the new findings will most likely reference the older research, but not the other way around. And since the author might not change his previous paper, the new information will not be accessible through this document even if they are semantically associated. If there was no distinction between authors and consumers concerning hyperlinks on these web documents, the community would easily be able to add a link to the new research.

\section{Coping} \label{sec:Coping}
	\paragraph{}
	The current web standards have not changed since the initial draft of HTML with regards to linking techniques because, as we discussed in the brief history of the web, the Web's evolution was dominantly influenced by commercial projects. The outdated techniques should have caused frustration with the users and in turn trigger an update that improved the quality of linking techniques but as of yet this has not occurred.
	\paragraph{}
	The linking techniques that were introduced with HTML were sufficient for its early intended use and when the Web became more popular with the general public, these standards were already in place. The Web and the Internet were new things to most of the users and learning to use the Web included learning how to handle and create hyperlinks on web pages. The early generation of web users were taught how to use the technology and not per se how to use the concept of hyperlinking. And since there was no alternative way of creating hyperlinks, this technique became standard.
	\paragraph{}
	The second generation was introduced to the Web by the first generation, including the linking technology. After some generations the way web pages are linked together was not questioned by the general public, it was something that has always been like that, and there were no real scenarios in which more functionality was required. Every now and then when more functionality was required a ad hoc hack or workaround would be put in place to mitigate the problem. In short, people learned to use the technology with its flaws and limitations simply because there was no known alternative and in general there was no need for more complex methods. By the time alternatives were popping up and better linking techniques became available, the general public got used to the old way of linking and saw no need to change their work flow.
	\paragraph{}
	The small group of users that did need more complex linking techniques could not really influence the course of the Web's evolution because commercial browsers are more inclined to put their efforts in satisfy the needs of the larger group of users.
